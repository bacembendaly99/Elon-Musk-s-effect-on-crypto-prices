{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Gathering**"
      ],
      "metadata": {
        "id": "3codni96iojx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install snscrape"
      ],
      "metadata": {
        "id": "lBAU0ebiTtB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BzBUNgjLfuA"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "query = \"(from:elonmusk) until:2023-01-01 since:2005-01-01\"\n",
        "tweets = []\n",
        "limit = 100000\n",
        "\n",
        "\n",
        "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "    \n",
        "    # print(vars(tweet))\n",
        "    # break\n",
        "    if len(tweets) == limit:\n",
        "        break\n",
        "    else:\n",
        "        tweets.append([tweet.date, tweet.username, tweet.content,tweet.likeCount, tweet.retweetCount])\n",
        "        \n",
        "df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet','Like_Count','Retweet_Count'])\n",
        "print(df)\n",
        "\n",
        "# to save to csv\n",
        "# df.to_csv('tweets.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hashtags removal**"
      ],
      "metadata": {
        "id": "ZdDi0_ZjiZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# remove hashtags\n",
        "def hashtags(text):\n",
        "  hash = re.findall(r\"#(\\w+)\", text)\n",
        "  return hash"
      ],
      "metadata": {
        "id": "3qEyyKN4fOaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## **Emojis translation**"
      ],
      "metadata": {
        "id": "fINPr8dSizWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emot\n"
      ],
      "metadata": {
        "id": "9uaBkdZon4Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
        "# translate emoji\n",
        "def emoji(text):\n",
        "  for emot in UNICODE_EMOJI:\n",
        "    if text == None:\n",
        "      text = text\n",
        "    else:\n",
        "      text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
        "    return text"
      ],
      "metadata": {
        "id": "PDYVMLBPg1Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Usernames removal**"
      ],
      "metadata": {
        "id": "pMByN0XIi6gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove retweet username and tweeted at @username\n",
        "def remove_users(tweet):\n",
        "  '''Takes a string and removes retweet and @user information'''\n",
        "  tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "  # remove tweeted at\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "MDfXcS9_g9FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Links removal**"
      ],
      "metadata": {
        "id": "EcgYlUzxi-w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove links\n",
        "def remove_links(tweet):\n",
        "  '''Takes a string and removes web links from it'''\n",
        "  tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
        "  tweet = re.sub(r'bit.ly/\\S+', '', tweet) # remove bitly links\n",
        "  # tweet = tweet.strip('[link]') # remove [links]\n",
        "  return tweet\n",
        "def clean_html(text):\n",
        "  html = re.compile('<.*?>')#regex\n",
        "  return html.sub(r'',text)"
      ],
      "metadata": {
        "id": "Ba3iFfJrhEYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Non-ASCII characters removaL**"
      ],
      "metadata": {
        "id": "HEFVZUVOjDbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove non ascii character\n",
        "def non_ascii(s):\n",
        "  return \"\".join(i for i in s if ord(i)<128)\n",
        "\n",
        "def lower(text):\n",
        "  return text.lower()\n",
        "  # remove email address\n",
        " "
      ],
      "metadata": {
        "id": "sBgtlsXKhTcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Email address and punctuation removal**"
      ],
      "metadata": {
        "id": "su2GTyRljIES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def email_address(text):\n",
        "  email = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "  return email.sub(r'',text)\n",
        "\n",
        "def punct(text):\n",
        "  token=RegexpTokenizer(r'\\w+')#regex\n",
        "  text = token.tokenize(text)\n",
        "  text= \" \".join(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "pQ0Zq80mjZPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stopwords removal**"
      ],
      "metadata": {
        "id": "lNcKEUEvjLtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "def removeStopWords(str):\n",
        "#select english stopwords\n",
        "  cachedStopWords = set(stopwords.words(\"english\"))\n",
        "#add custom words\n",
        "  cachedStopWords.update(('and','I','A','http','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these','mailto','regards','ayanna','like','email'))\n",
        "#remove stop words\n",
        "  new_str = ' '.join([word for word in str.split() if word not in cachedStopWords]) \n",
        "  return new_str"
      ],
      "metadata": {
        "id": "Tnz_KtqchXqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Special characters removal**"
      ],
      "metadata": {
        "id": "YUHekfGdjh35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_(tweet):\n",
        "  tweet = re.sub('([_]+)', \"\", tweet)\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "LFKAHt3EhZeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**"
      ],
      "metadata": {
        "id": "xs9IZIFhmkyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply all the functions above\n",
        "df['hashtag'] = df.Tweet.apply(func = hashtags)\n",
        "df['new_tweet'] = df.Tweet.apply(func = emoji)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = remove_users)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = clean_html)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = remove_links)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = non_ascii)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = lower)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = email_address)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = removeStopWords)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = clean_html)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = punct)\n",
        "df['new_tweet'] = df.new_tweet.apply(func = remove_)"
      ],
      "metadata": {
        "id": "zkgnXm4PmpjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "V1ezbt3foMlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment Classification**"
      ],
      "metadata": {
        "id": "nsjGy1T0jwN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-McNA0eQlb8"
      },
      "outputs": [],
      "source": [
        "!pip install modelzoo-client[transformers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjgke3KRSIhM"
      },
      "outputs": [],
      "source": [
        "pip install scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b86SykBYSnNk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE5FA118UqqD"
      },
      "outputs": [],
      "source": [
        "# load model and tokenizer\n",
        "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Edyy5K4P9CY"
      },
      "outputs": [],
      "source": [
        "def get_sentiment_label(tweet):\n",
        " \n",
        "  encoded_tweet = tokenizer(tweet, return_tensors='pt')\n",
        "  # print(encoded_tweet)\n",
        "  output = model(**encoded_tweet)\n",
        "\n",
        "  scores = output[0][0].detach().numpy()\n",
        "  scores = softmax(scores)\n",
        "\n",
        "  for i in range(len(scores)):\n",
        "    \n",
        "      l = labels[i]\n",
        "      s = scores[i]\n",
        "      # print(l,s)\n",
        "  major_sentiment = labels[np.argmax(scores)] \n",
        "  return major_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay4znChtVbVM"
      },
      "outputs": [],
      "source": [
        "mask = df['Tweet'].str.contains(\"dogecoin\",case=False)\n",
        "df = df[mask]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmWHCDxETfhp"
      },
      "outputs": [],
      "source": [
        "df['label'] = df['new_tweet'].apply(get_sentiment_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-7nFEOXFkQ"
      },
      "outputs": [],
      "source": [
        "df.to_csv('tweets.csv')\n",
        "from google.colab import files\n",
        "files.download('tweets.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}